{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15f1622d",
   "metadata": {},
   "source": [
    "## Mecabのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f121817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "875591b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents\n",
    "doc0 = \"\"\"\n",
    "本研究では、Recurrent Neural Network（RNN）ベースのシーケンスモデルであるSummaRuNNerを発表し、最先端の技術よりも優れた、あるいは同等の性能を達成することを示した。==\n",
    "私たちのモデルは、情報量、重要性、新しさなどの抽象的な特徴に基づいて予測を視覚化できるため、非常に解釈しやすいという利点もあります。==\n",
    "もう一つの新しい貢献は、抽出モデルの抽象化学習で、人間が生成した参考文献の要約だけで学習することができ、文レベルの抽出ラベルを必要としません。==\n",
    "文書の要約は、情報検索や自然言語理解に多くの応用がある重要な問題である。==\n",
    "要約技術は主に抽出型と抽象型の2つに分類されます。==\n",
    "抽出型の手法は、文書の中から注目すべきスニペット、センテンス、またはパッセージを選択することを目的とし、抽象型の要約手法は、文書の中の情報内容を簡潔に言い換えることを目的としています。==\n",
    "文書の要約に関する文献の大部分は、抽出型の要約に費やされている。==\n",
    "従来の抽出型要約の手法は、大きく分けて、欲張りなアプローチ（例：(Carbonell and Goldstein 1998)）、グラフベースのアプローチ（例：(Radev and Erkan 2004)）、制約最適化ベースのアプローチ（例：(McDonald 2007)）に分類される。==\n",
    "近年、ニューラルネットワークを利用した抽出型の要約手法が注目されています。==\n",
    "例えば、(Kageback et al. 2014)は、再帰的オートエンコーダー(Socher et al. 2011)を採用して文書を要約し、Opinosisデータセット(Ganesan, Zhai, and Han 2010)で最高の性能を発揮しました。==\n",
    "Yin and Pei 2015）は、畳み込みニューラルネットワーク（CNN）を適用して、文章を連続ベクトル空間に投影し、「威信」と「多様性」に基づいてコストを最小化することで文章を選択し、マルチドキュメント抽出型要約のタスクに適用した。==\n",
    "もう1つの関連研究は、CNNを使ったクエリに焦点を当てたマルチドキュメントの要約の問題に取り組んでいる(Cao et al. 2016)で、ここではドキュメントを表現するために文表現に対する重み付き和プーリングを使用している。==\n",
    "重みは、クエリに基づく文の表現に対する注意から学習されます。==\n",
    "最近では、テキスト用の強力な生成ニューラルモデルが登場したこともあり（Bahdanau, Cho, and Bengio 2014）、抽象化技術も普及しつつあります。==\n",
    "例えば、(Rush, Chopra, and Weston 2015)は、文を短い見出しに抽象的に要約するための注意フィードフォワードネットワークを提案した。==\n",
    "彼らの研究をさらに発展させた(Nallapati, Zhou, and Xiang 2016)は、リカレントニューラルネットワークベースのエンコーダ・デコーダモデルのセットを提案しています。このモデルは、語彙力のない単語の処理や、文中の単語の構文的特徴のモデル化など、要約のさまざまな側面に焦点を当てています。==\n",
    "また、後続の研究(Nallapati et al. 2016)では、CNN/DailyMailコーパスを用いて、大規模文書を複数文の要約にまとめる抽象化技術を提案しています。==\n",
    "抽象化技術が登場したにもかかわらず、抽出技術は、複雑さやコストが少なく、文法的・意味的に正しい要約を生成することができるため、依然として魅力的です。==\n",
    "ごく最近の研究では、ChengとLapata（2016）が、抽出的な単一文書の要約のためのアテンション・エンコーダ・デコーダを提案し、CNN/Daily Mailコーパスに適用した。==\n",
    "(Cheng and Lapata 2016)と同様に、我々の研究もニューラルネットワークを用いた単一文書のセンテンス抽出型要約のみに焦点を当てています。我々の実験には、(Nallapati et al. 2016)と(Cheng and Lapata 2016)が使用したのと同じコーパスを使用していますが、これはその大きなサイズが我々のような数千個のパラメータを持つ深層ニューラルネットワークの学習に魅力的だからです。==\n",
    "我々の主な貢献は以下の通りである。(a) シンプルなリカレントネットワークベースのシーケンス分類器であるSummaRuNNerを提案し、抽出型要約のための最新モデルを凌駕またはマッチさせる。==\n",
    "SummaRuNNerは、最先端の性能に加えて、非常に解釈しやすいという利点があります。==\n",
    "分類層の項が明確に分かれているため（式6参照）、各文の分類に関わる様々な要因を解明することができます。==\n",
    "これを図2に示します。ここでは、検証セットの代表的な文書と、その最終的な分類を担当した各抽象的特徴の正規化スコアを表示しています。==\n",
    "このような視覚化は、システムが行った決定をエンドユーザーに説明する際に特に有効です。==\n",
    "また，Daily MailコーパスとDUCコーパスからいくつかの例文を表示し，SummaRuNNerが選んだ文を強調して，表5の金色の要約と比較した．==\n",
    "これらの例は、SummaRuNNerがドキュメントのキーポイントを特定する上で適度に良い仕事をすることを定性的に示しています。==\n",
    "本研究では、直感的な可視化が可能な抽出型文書要約のための非常に解釈可能なニューラルシーケンスモデルを提案し、最先端の深層学習モデルよりも優れた性能を発揮する、あるいは同等の性能を発揮することを示します。==\n",
    "また、学習時にラベルを抽出する必要をなくすために、新しい抽象的な学習メカニズムを提案していますが、このアプローチは、ほとんどのデータセットにおいて、私たちの抽出的な学習に比べて、まだ2、3のルージュポイントしかありません。==\n",
    "今後は、抽出的アプローチと抽象的アプローチの組み合わせをさらに検討していく予定です。==\n",
    "単純なアプローチとしては、抽象的なトレーニングを用いて抽出モデルを事前にトレーニングすることが考えられます。==\n",
    "さらに、抽出部の予測値が抽象部で消費される確率的な中間ユニットを形成するような、抽出・抽象の合同モデルの構築を計画しています。==\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b80cd9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Salience Estimation via Variational Auto-Encoders for Multi-Document Summarization\n",
    "doc1 = \"\"\"\n",
    "本研究では、マルチドキュメントサマリゼーション（MDS）のための新しい教師なし文サリエンスフレームワークを提案する。このフレームワークは、潜在的意味モデリングとサリエンス推定の2つのコンポーネントに分けられる。==\n",
    "潜在的意味モデリングでは、観測された文とそれに対応する潜在的意味表現を記述するために、Variational Auto-Encoders (VAEs) と呼ばれるニューラル生成モデルを採用する。==\n",
    "また、潜在変数の事後推論には、ニューラル変分推論を用いる。==\n",
    "本研究では、潜在的意味空間と観測された項ベクトル空間の再構成を共同で考慮する、教師なしデータ再構成フレームワークを提案する。==\n",
    "これは、潜在的な意味空間と観測された用語のベクトル空間の再構成を共同で行うものであり、これら2つの異なる補完的なベクトル空間から文のサリエンスを捉えることができる。==\n",
    "その後，VAEsに基づく潜在的意味モデルを文の重要度推定コンポーネントに統合し，マルチタスク学習によるバックプロパゲーションによってフレームワーク全体を共同で学習することができる。==\n",
    "ベンチマークデータであるDUCとTACを用いた実験の結果、本研究のフレームワークは、最先端のモデルよりも優れた性能を達成した。==\n",
    "マルチドキュメントサマリゼーション（MDS）は、異なるソースからのドキュメントセットでイベントを記述するトピックに対して、簡潔でよくまとまったサマリを自動的に生成することを目的としており、広く研究されています。==\n",
    "(Goldstein et al. 2000; Erkan and Radev 2004; Wan, Yang, and Xiao 2007; Nenkova and McKeown 2012; Min, Chew, and Tan 2012; Bing et al. 2015)。==\n",
    "Summarizationアプローチは、抽出ベースの手法と抽象化ベースの手法の2つのクラスに分類されます。==\n",
    "どちらのクラスでも、パフォーマンスを向上させるためには、サリエンス推定が重要な役割を果たします。==\n",
    "ラベリングされたMDSデータセットのスケーラビリティの制約を考慮して、いくつかの作品では、教師なしのデータ再構成法を採用してサリエンス推定を行い、同等の結果を得ています（He et al 2012; Liu, Yu, and Deng 2015; Yao, Wan, and Xiao 2015; Li et al 2015; Ren et al 2016; Song et al 2017）。==\n",
    "これらの作品を調査した結果、主にBag-of-Words (BoWs) ベクトルを文の表現と再構成損失関数に使用していることがわかりました。==\n",
    "一方、いくつかの研究成果（Le and Mikolov 2014; Kim 2014）では、文や文書のセマンティクスをモデル化する際に、分散表現がBoWを上回ることが実証されています。==\n",
    "本論文では、BoWsベクトルを使用する代わりに、MDSタスクのために文の潜在的なセマンティクスをモデル化するための分散表現を探ります。==\n",
    "我々は、観測された文と潜在的な意味ベクトルを記述するために、確率的な生成モデルに基づいたフレームワークを提案する。==\n",
    "一連の文書で構成されるトピック（イベント）が与えられたとき、各文をモデル化するための分散潜在的意味ベクトルを生成的フレームワークで構築し、各文は観測されていない潜在的意味空間から生成されます。==\n",
    "もう一つの特徴は、生成過程において、潜在的な意味ベクトル上の分布を近似するために、入力テキストを条件としたニューラルネットワークを採用していることです。==\n",
    "Markov Chain Monte Carlo（MCMC）サンプリングとVariational Inference（VI）は、生成モデルで使用される最も一般的な手法です（Jordan et al 1999; Wainwright and Jordan 2008; Blei, Kucukelbir, and McAuliffe 2016）。==\n",
    "しかし、連続的な潜在変数やニューラルネットワークベースの生成モデリングのために、限界尤度の積分の一部は難解です。==\n",
    "平均場アルゴリズム（Xing, Jordan, and Russell 2002）のような標準的な変分推論法は使用できません。==\n",
    "さらに、MCMCベースのサンプリング手法は、大規模な機械学習タスクに拡張するには遅すぎます。==\n",
    "最近では、複雑な生成モデリングフレームワークに関連する推論問題を扱うことができる Variational Autoencoders (VAEs) (Kingma and Welling 2014; Rezende, Mohamed, and Wierstra 2014) や Generative Adversarial Networks (GANs) (Goodfellow et al. 2014; Radford, Metz, and Chintala 2015) が提案されている。==\n",
    "私たちの研究では、生成モデルの基本的な枠組みとしてVAEを採用しています。==\n",
    "実際、いくつかの作品（Miao, Yu, and Blunsom 2015; Chung et al. 2015）では、VAEが一般的なリカレントニューラルネットワーク（RNN）やコンボリューショナルニューラルネットワーク（CNN）よりも高レベルの意味表現を生成する能力が高いことが実証されています。==\n",
    "MDSにおける文の重要度推定問題を解決するために、我々は潜在的意味空間と観測された用語ベクトル空間を共同で再構成する教師なしデータ再構成フレームワークを提案する。==\n",
    "データ再構成の基本的な考え方は、各原文は、他のいくつかの代表的な文の線形結合を用いて再構成できるというものです。==\n",
    "これらの代表的な文章は、「何が起こったか」、「損害」、「対策」など、事象に込められた様々な側面を捉えることができます。==\n",
    "アスペクト文を表現するためのベクトルを、アスペクト・ベクトルと名付けます。==\n",
    "そして、アスペクト・ベクトルを用いて、再構成の際にサルーン推定を行うことができます。==\n",
    "生成モデルとデータ再構築プロセスの精神に基づいて、いくつかの潜在的なアスペクト・ベクトルを設計し、それらを使って元の潜在的な意味空間全体を再構築します。==\n",
    "このようなアイデアと並行して、元の観測された用語ベクトル空間を再構築するために使用されるいくつかのアスペクト用語ベクトルも設計します。==\n",
    "その後、VAEsに基づく潜在的意味モデルを文の重要度推定コンポーネントに統合し、マルチタスク学習によるバックプロパゲーションによってフレームワーク全体を共同で学習することができます。==\n",
    "文の重要度を推定した後、フレーズの結合に基づく統一的な最適化フレームワークを採用し、最終的な要約を生成します。==\n",
    "我々の貢献は以下の通りです。(1)文の潜在的意味モデル化を行うためのVAEベースの生成モデルを提案する。==\n",
    "私たちの知る限りでは、要約関連のタスクにVAEを使用することを検討している他の研究はありません。==\n",
    "(2) 本フレームワークでは、潜在的な意味空間と観測された入力項のベクトル空間を共同で考慮することで、これらの異なる補完的な空間からより豊かな情報を引き出すことができる、ということです。==\n",
    "(3)VAEsベースの生成モデルとサリエンス推定コンポーネントを統一されたフレームワークに統合し、バックプロパゲーションを用いたマルチタスク学習で同時に学習できるようにした。==\n",
    "(4) ベンチマークデータセットであるDUCとTACでの実験結果は、本フレームワークが最先端のモデルよりも優れた性能を達成することを示している。==\n",
    "私たちのフレームワークの性能を過去の手法と比較するためには、まず、彼らのシステムが作成したサマリーを入手すること（または、彼らのコードを入手して自分たちでサマリーを作成すること）が優先されます。==\n",
    "そして、同じオプションでROUGE評価を行います。==\n",
    "我々のシステムを、いくつかの要約のベースラインや既存の教師なしの手法と比較した。==\n",
    "ランダムベースラインは、各トピックの文章をランダムに選択します。==\n",
    "Lead baseline (Wasson 1998) は、ニュースを時系列にランク付けし、先頭のセンテンスを1つずつ抽出します。==\n",
    "また、スパースコーディングに基づく他の3つの教師なしの既存手法、すなわちDSDR（He et al 2012）、MDS-Sparse（Liu, Yu, and Deng 2015）、RA-MDS（Li et al 2015）も比較しています。==\n",
    "ABSPhrase (Bing et al. 2015) は、フレーズベースの最適化フレームワークを用いて、重み付けされた用語の頻度をサリエンスの推定値として、抽象的なサマリーを生成します。==\n",
    "さらに、SpOpt(Yao, Wan, and Xiao 2015)も論文の中で良い結果を提示しているが、彼らのシステムを忠実に再現するように再構築することは困難であることを言及しておきたい。==\n",
    "表1および表2に示すように，本システムはすべてのROUGE指標において最良の結果を達成した．==\n",
    "VAEによる潜在的なセマンティックモデリングと共同でのセマンティック空間の再構築により、MDSの性能が大幅に向上することが実証されました。==\n",
    "VAEs-Zeroも同等の性能を達成していることは注目に値します。==\n",
    "VAEs-Aには及ばないものの、既存のほとんどの手法よりも優れています。==\n",
    "そのため、VAEに基づく潜在的なセマンティックモデリングは、MDSのパフォーマンスを向上させることができます。==\n",
    "これらの教師なしモデルの他に、我々の知る限りでは、Wang et al (2013)で発表された方法がDUC 2007で最高のパフォーマンスを達成しました。==\n",
    "その理由は、文章圧縮モデルと文書要約モデルの学習に、教師あり学習のフレームワークを使用しているからです。==\n",
    "評価では、教師あり学習に基づく2つの文選択法を提供しています。Support Vector Regression (SVR)とLambdaMARTです。==\n",
    "SVRはRouge-2で0.095、Rouge-SU4で0.147を得ています。==\n",
    "LambdaMARTは、0.123と0.156を得ています。==\n",
    "教師なしの我々のフレームワークは、SVRよりも優れており、LambdaMARTと比較しても同様の結果を得ることができました。==\n",
    "データセットTAC 2011において、上記のベースラインの他に、我々のフレームワークをいくつかのトップシステムと比較した。PKUTM (Li et al. 2011) は，文のスコアリングと選択に manifoldranking を採用しているが，表3に示すように，我々の性能は PKUTM よりも優れている．==\n",
    "PKUTMがドメイン知識の提供にWikipediaコーパスを使用したことは注目に値します。==\n",
    "SWING（Min, Chew, and Tan 2012）という手法は、TAC 2011のシステムの中で最も優れたものです。==\n",
    "しかし、我々の結果はSWINGには及ばない。==\n",
    "その理由は、SWINGがカテゴリー別の特徴量を使い、TAC2010データのカテゴリー情報を用いて、特徴量の重みを教師付きで学習しているからである。==\n",
    "これらの機能は、要約のためのより良いカテゴリー別のコンテンツを選択するのに役立ちます。==\n",
    "対照的に、我々のモデルは教師なしで、TAC 2010を一般的なパラメータチューニングの目的でのみ使用しています。==\n",
    "我々は、SzとSxがイベントの異なる側面を表すことに言及している。==\n",
    "このアイデアを検証するために、TAC2011で「Pet Food Recall」というトピックを取り上げ、各アスペクトからいくつかのキーワードを抽出します。==\n",
    "アスペクト-1は「Nutro、購入、ダース、ドロップ、60、タイミング、プロテイン、研究」、アスペクト-2は「Sarah、Tuite、ソース、プロテイン、Food、and、Drug Administration」、アスペクト-3は「食品、会社、リコール、ペット、メニュー、猫、製品、フード、犬」という言葉を含んでいます。==\n",
    "これは、私たちのフレームワークがトピックの主要な側面を捉えることができることを示しています。==\n",
    "さらに、Sxの大きさが、単語のサリエンス情報を表すこともわかりました。==\n",
    "TAC2011から3つのトピックを選びます。\"アーミッシュ・シューティング」、「四川大地震」、「ペットフード・リコール」。==\n",
    "各トピックについて，表4に示すように，辞書の用語をsalience scoreに従ってソートし，上位10個の用語を抽出する。==\n",
    "上位10位までの用語には、各トピックの最も重要な情報が含まれていることがわかります。==\n",
    "アーミッシュ・シューティング」というトピックでは、ゴールデン・サマリーの一文に注目です。\"On October 2, 2006, a gunman, Charles Roberts, entered an Amish school near Lancaster, PA, and took the children hostage, killed five girls and wounded other children before suicide himself.\" トップ10の用語が主要なセマンティクスを捉えることができるのは明らかです。==\n",
    "我々は、新しい教師なしのマルチドキュメントサマリゼーション（MDS）フレームワークを提案する。==\n",
    "まず、VAEsベースの生成モデルを用いて、文を用語ベクトル空間から潜在的意味空間にマッピングします。==\n",
    "そして、潜在的な意味空間と観測された用語ベクトル空間を、アスペクトに関連するベクトルを用いて共同で再構成することで、教師なしのデータ再構成モデルを提案し、サリエンス推定を行う。==\n",
    "ベンチマークデータセットであるDUCとTACでの実験結果は、我々のフレームワークが最先端のモデルよりも優れた性能を達成していることを示している。==\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55304299",
   "metadata": {},
   "outputs": [],
   "source": [
    "### A Graph Based Approach on Extractive Summarization\n",
    "doc2 = \"\"\"\n",
    "情報技術とインターネットの出現により、世界は毎秒数テラバイトの情報を生み出しています。==\n",
    "過去10年の間に、ある事件をほぼ瞬時に報告するオンラインニュースフィードがいくつも登場しています。==\n",
    "このため、コンテンツを減らし、要約と呼ばれる必要なものだけをユーザーに提示することが切実に求められています。==\n",
    "本稿では、グラフ理論に基づく抽出的要約化手法を提案する。==\n",
    "この手法では、文書全体をグラフ化した後、infomapクラスタリングを用いて最も情報量の多い文章を見つけることで、文書全体の代表的な要約や要約を作成しようとする。==\n",
    "要約システムは、フルサイズの記事を縮小して、関連する情報とともにその文章の主要なアイデアや中心的なアイデアを伝える流暢で簡潔な要約を作成するために機能します。==\n",
    "要約とは、1つ以上のテキストから生成され、元のテキストに含まれる情報のかなりの部分を含み、元のテキストの半分以下の長さのテキストのことです。==\n",
    "システムは、段落長の要約を生成します。==\n",
    "自動文書要約は、Luhnの研究[18]にまでさかのぼる。==\n",
    "原文から重要な概念を抽出し、中間表現を構築するために、多くの手法[16][22][8]が提案されてきた。==\n",
    "初期の手法[5][10]は、主に統計的な性質を持つもので、文書内の最も重要な概念を決定するために単語の頻度に焦点を当てていました。==\n",
    "このような統計的アプローチの反対の極端な方法は、原文の真の「意味理解」を試みることです。==\n",
    "深い意味分析の使用は、質の高い要約を作成するための最良の機会を提供します。==\n",
    "このようなアプローチの問題点は、詳細な意味表現が作成されなければならず、ドメイン固有の知識ベースが利用可能でなければならないことです。==\n",
    "本論文では、いくつかの種類の要約技術があるが、抽出的要約技術についてのみ論じている。==\n",
    "抽出的要約器は基本的に3つの比較的独立したフェーズで動作する。==\n",
    "すべての要約器は、最初にテキストの中間的な表現を生成するように動作する。==\n",
    "生の入力は、ストップワード、すなわち、頻繁に使用される記事、接続詞、接続詞を除去するために前処理されます。==\n",
    "頻繁に使用される冠詞、接続詞、前置詞などを除去します。==\n",
    "場合によっては、句読点も削除されます。==\n",
    "残りの部分から、しばしば「用語頻度」や「逆文書頻度」、あるいはその両方が発見され、キーと値のペアとして保存されます。==\n",
    "これらのメトリクスは、文書内の情報の核心を理解し、特定の方法で処理するのに役立ちます。==\n",
    "次に、中間表現に基づいて文章に重みが割り当てられます。==\n",
    "最終段階では、貪欲なアプローチで最も高いスコアの文章が選択されます。==\n",
    "これらの段階については、後続のセクションで徹底的に説明する。==\n",
    "この方法は、BBCニュースのフィードで生成されたニュース記事に対して徹底的に検証しています。==\n",
    "処理にはNLTKライブラリ[4]とPython 2.7[2]のBeautifulSoupライブラリを使用しています。==\n",
    "本論文では、抽出的要約を実現するための新しいグラフベースのアプローチを提示することを試みた。==\n",
    "文から名前のついた実体を削除することで、文のペアごとの類似度測定値に偏りがなくなり、重要な単語が強調されるようになる。==\n",
    "この処理は、ニュース記事を対象にして評価すると、興味深い結果が得られる。==\n",
    "将来的には、名詞-原音分離を改良して、より良い結果が得られるようにしたいと考えています。==\n",
    "また、Infomap のクラスタリングアルゴリズムの代わりに、グラフベースのクラスタリングアルゴリズムやその他のクラスタリングアルゴリズムを適用することも可能であり、将来的には比較検討を行う予定である。==\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "516b2deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LexRank: Graph-based Lexical Centrality as Salience in Text Summarization\n",
    "doc3 = \"\"\"\n",
    "自然言語処理におけるテキスト単位の相対的重要度を計算するための確率的グラフベースの手法を紹介する。==\n",
    "本手法は、テキスト要約(TS)の問題に対してテストを行う。==\n",
    "抽出的TSは、文書または文書の集合の中で最も重要な文を識別するために、文のサリエンスの概念に依存している。==\n",
    "サリエンスは通常、特定の重要な単語の存在、またはセントロイド疑似文との類似性の観点から定義される。==\n",
    "我々は、文のグラフ表現における固有ベクトル中心性の概念に基づいて文の重要度を計算するための新しいアプローチ、LexRankを考えている。==\n",
    "このモデルでは、文のグラフ表現の隣接行列として、文内余弦類似度に基づく接続性行列を用いる。==\n",
    "本システムは、LexRankをベースにしており、最近のDUC2004の評価では、1つ以上のタスクで1位を獲得した。==\n",
    "この論文では、我々のアプローチの詳細な分析を行い、以前のDUC評価のデータを含むより大きなデータセットに適用した。==\n",
    "また、類似度グラフを用いて中心性を計算する方法をいくつか紹介する。==\n",
    "その結果、ほとんどの場合において、学位ベースの手法（LexRankを含む）がセントロイドベースの手法とDUCに参加している他のシステムの両方を凌駕していることがわかった。==\n",
    "さらに，閾値付きLexRank法は，連続LexRankを含む他の度ベースの手法よりも優れていることがわかった．==\n",
    "また、我々のアプローチは、文書の不完全な局所的クラスタリングに起因するデータ中のノイズに対して非常に鈍感であることを示す。==\n",
    "近年、自然言語処理(NLP)は非常に堅固な数学的基盤へと移行しています。==\n",
    "NLPにおける多くの問題、例えば、構文解析(Collins, 1997)、語義曖昧性解消(Yarowsky, 1995)、自動言い換え(Barzilay & Lee, 2003)などは、ロバストな統計的手法の導入によって大きな恩恵を受けてきました。==\n",
    "最近では、ロバストなグラフベースのNLP手法も注目されており、例えば、単語クラスタリング(Brew & im Walde, 2002)や前置詞句添付(Toutanova, Manning, & Ng, 2004)などが挙げられる。==\n",
    "この論文では、NLPにおけるグラフベースの手法をさらに一歩進めていく。==\n",
    "文ベースのグラフ上でのランダムウォークがテキストの要約にどのように役立つかを議論する。==\n",
    "また、同様の手法が、名前付き実体分類、前置詞句の添付、テキスト分類（例：スパム認識）などの他のNLPタスクにどのように適用できるかについても簡単に議論する。==\n",
    "テキスト要約は、ユーザにとって有用な情報を提供する、与えられたテキストの圧縮版を自動的に作成するプロセスです。==\n",
    "要約の情報内容は、ユーザのニーズに依存する。==\n",
    "トピック指向の要約は、ユーザの興味のあるトピックに焦点を当て、指定されたトピックに関連するテキスト内の情報を抽出する。==\n",
    "一方、一般的な要約は、原文の一般的なトピック構成を維持したまま、できるだけ多くの情報内容をカバーしようとする。==\n",
    "本論文では、同じだが特定されていないトピックに関する複数の文書の要約を生成することを目的とした、複数文書の抽出的汎用要約に焦点を当てている。==\n",
    "抽出的要約は、元の文書に含まれる文章のサブセットを選択して要約を生成する。==\n",
    "これは、テキスト内の情報が言い換えられる抽象的要約とは対照的です。==\n",
    "人間によって生成される要約は一般的に抽出的ではありませんが、今日の要約研究のほとんどは抽出的要約に関するものです。==\n",
    "純粋に抽出的な要約は、自動抽象化された要約と比較して、より良い結果が得られることが多い。==\n",
    "これは、意味表現、推論、自然言語生成などの抽象的要約の問題が、文の抽出のようなデータ駆動型のアプローチに比べて比較的難しいという事実に起因している。==\n",
    "実際、真に抽象的な要約は、今日では成熟した段階には至っていない。==\n",
    "既存の抽象的要約器は、多くの場合、抽出的前処理コンポーネントに依存している。==\n",
    "抽出器の出力は、テキストの要約を生成するためにカットアンドペースト、または圧縮されます(Witbrock & Mittal, 1999; Jing, 2002; Knight & Marcu, 2000)。==\n",
    "SUMMONS (Radev & McKeown, 1998)は、複数のソースから情報を抽出・結合し、この情報を言語生成コンポーネントに渡して最終的な要約を生成する複数文書要約器の一例である。==\n",
    "抽出的要約に関する初期の研究は、テキスト内の文の位置、それらが含む単語の全体的な頻度、または文の重要性を示すいくつかのキーフレーズのような文の単純なヒューリスティックな特徴に基づいています(Baxendale, 1958; Edmundson, 1969; Luhn, 1958)。==\n",
    "文中の単語の重要性を評価するために一般的に使用されている尺度は、逆文書度数（idf）であり、式（Sparck-Jones, 1972）で定義されています。==\n",
    "ここで、N はコレクションに含まれる文書の総数であり、 $n_i$ は単語 i が出現する文書の数です。==\n",
    "例えば、ほぼすべての文書に出現する可能性のある単語（例えば、記事 \"a \"や \"the\"）は、idf値がゼロに近いのに対し、まれな単語（例えば、医学用語や固有名詞）は、一般的に高いidf値を持っています。==\n",
    "より高度な技術では、単語の同義語やアナフォラ解決を用いて、文と文の関係や談話構造を考慮することもある(Mani & Bloedorn, 1997; Barzilay & Elhadad, 1999)。==\n",
    "また、より多くの特徴が提案され、より多くの訓練データが利用可能になったため、研究者は機械学習を要約に統合しようとしてきた(Kupiec, Pedersen, & Chen, 1995; Lin, 1999; Osborne, 2002; Daum е III & Marcu, 2004)。==\n",
    "本論文では、クラスタ内の各文の中心性を評価し、最も重要な文を抽出して要約に含めることを目的としている。==\n",
    "本論文では、複数文書の要約において、文の語彙的特性の観点から中心性を測定する「語彙的中心性の原理」の定義方法について検討する。==\n",
    "第2節では、文の中心性を判定する方法としてよく知られているセントロイドベースの要約法を紹介し、次に、文の中心性を判定する3つの新しい尺度を紹介する。==\n",
    "第2節では，文の中心性を判定する方法としてよく知られているセントロイドベースの要約法を紹介した後，ソーシャルネットワークにおける「プレステージ」の概念にヒントを得て，Degree，閾値付きLexRank，連続的LexRankの3つの新しい尺度を紹介する。==\n",
    "我々は、文書クラスタのグラフ表現を提案し、頂点は文を表し、辺は文のペア間の類似関係の観点から定義する。==\n",
    "この表現により、グラフ上で定義されたいくつかの中心性ヒューリスティックを利用することができる。==\n",
    "我々は、特徴ベースの汎用要約ツールキットMEADを用いて、我々の新しい手法をセントロイドベースの要約と比較し、我々の新しい特徴がほとんどの場合でセントロイドよりも優れていることを示した。==\n",
    "実験のためのテストデータは、2003年と2004年に行われた文書理解会議(DUC)の要約評価から得られたものであり、他の最新の要約システムや人間の性能と比較することができる。==\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eca19b1",
   "metadata": {},
   "source": [
    "## テキストの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05b13609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# 不要なテキストの削除（前処理）\n",
    "def preprocessing(text):\n",
    "  # 全角 => 半角\n",
    "  text = text.translate(str.maketrans({chr(0xFF01 + i): chr(0x21 + i) for i in range(94)}))\n",
    "  # 不要な単語の除去\n",
    "  text = text.replace('==', '')\n",
    "  # 記号をスペースで置換し、小文字化\n",
    "  #table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "  #text = text.translate(table).lower()\n",
    "  # 数字列を0に置換\n",
    "  text = re.sub('[0-9]+', '0', text)\n",
    "  return text\n",
    "\n",
    "doc0_splt = [s for s in preprocessing(doc0).split(\"\\n\") if s]\n",
    "doc1_splt = [s for s in preprocessing(doc1).split(\"\\n\") if s]\n",
    "doc2_splt = [s for s in preprocessing(doc2).split(\"\\n\") if s]\n",
    "doc3_splt = [s for s in preprocessing(doc3).split(\"\\n\") if s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7147f20",
   "metadata": {},
   "source": [
    "## DataFrame化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e1e3d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>そのため、VAEに基づく潜在的なセマンティックモデリングは、MDSのパフォーマンスを向上させ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>その理由は、文章圧縮モデルと文書要約モデルの学習に、教師あり学習のフレームワークを使用してい...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>既存の抽象的要約器は、多くの場合、抽出的前処理コンポーネントに依存している。</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>私たちの知る限りでは、要約関連のタスクにVAEを使用することを検討している他の研究はありません。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>抽出型の手法は、文書の中から注目すべきスニペット、センテンス、またはパッセージを選択すること...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>過去0年の間に、ある事件をほぼ瞬時に報告するオンラインニュースフィードがいくつも登場しています。</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>データセットTAC 0において、上記のベースラインの他に、我々のフレームワークをいくつかのト...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>(Goldstein et al. 0; Erkan and Radev 0; Wan, Y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>要約とは、0つ以上のテキストから生成され、元のテキストに含まれる情報のかなりの部分を含み、元...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>VAEによる潜在的なセマンティックモデリングと共同でのセマンティック空間の再構築により、MD...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  target\n",
       "0    そのため、VAEに基づく潜在的なセマンティックモデリングは、MDSのパフォーマンスを向上させ...       1\n",
       "1    その理由は、文章圧縮モデルと文書要約モデルの学習に、教師あり学習のフレームワークを使用してい...       1\n",
       "2               既存の抽象的要約器は、多くの場合、抽出的前処理コンポーネントに依存している。       3\n",
       "3     私たちの知る限りでは、要約関連のタスクにVAEを使用することを検討している他の研究はありません。       1\n",
       "4    抽出型の手法は、文書の中から注目すべきスニペット、センテンス、またはパッセージを選択すること...       0\n",
       "..                                                 ...     ...\n",
       "183   過去0年の間に、ある事件をほぼ瞬時に報告するオンラインニュースフィードがいくつも登場しています。       2\n",
       "184  データセットTAC 0において、上記のベースラインの他に、我々のフレームワークをいくつかのト...       1\n",
       "185  (Goldstein et al. 0; Erkan and Radev 0; Wan, Y...       1\n",
       "186  要約とは、0つ以上のテキストから生成され、元のテキストに含まれる情報のかなりの部分を含み、元...       2\n",
       "187  VAEによる潜在的なセマンティックモデリングと共同でのセマンティック空間の再構築により、MD...       1\n",
       "\n",
       "[188 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "doc0_df = pd.DataFrame({'text': doc0_splt, 'target': 0})\n",
    "doc1_df = pd.DataFrame({'text': doc1_splt, 'target': 1})\n",
    "doc2_df = pd.DataFrame({'text': doc2_splt, 'target': 2})\n",
    "doc3_df = pd.DataFrame({'text': doc3_splt, 'target': 3})\n",
    "\n",
    "df = pd.concat([doc0_df, doc1_df, doc2_df, doc3_df]).sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b6de3",
   "metadata": {},
   "source": [
    "## データを眺める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d97df6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "728cb51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'target'}>]], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87255f5",
   "metadata": {},
   "source": [
    "## 単語埋め込みのために、学習用と検証用に分ける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1df89ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rate = 0.8\n",
    "train_num = int(len(df)*train_rate)\n",
    "train_df = df.copy()[:train_num]\n",
    "test_df = df.copy()[train_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32064f30",
   "metadata": {},
   "source": [
    "## TF-IDFによる単語埋め込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf14f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import MeCab\n",
    "\n",
    "def tokenize(text, target_pos=['名詞', '形容詞', '形容動詞', '動詞', '副詞']):\n",
    "  tokens = []\n",
    "  mecab = MeCab.Tagger ()\n",
    "  mecab.parse('') #文字列がGCされるのを防ぐ\n",
    "  node = mecab.parseToNode(text)\n",
    "  while node:\n",
    "    #単語を取得\n",
    "    word = node.surface\n",
    "    #品詞を取得\n",
    "    pos = node.feature.split(\",\")[0]\n",
    "    # 名詞の場合のみ抽出\n",
    "    if pos in target_pos:\n",
    "      tokens.append(word)\n",
    "    #次の単語に進める\n",
    "    node = node.next\n",
    "  return tokens\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 1))\n",
    "vectors = vectorizer.fit_transform(train_df.text)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDFによるベクトル化\n",
    "vectors_test = vectorizer.transform(test_df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c31703d",
   "metadata": {},
   "source": [
    "## ベクトルをDFに変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55648545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトルをデータフレームに変換\n",
    "train_text_df = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names())\n",
    "test_text_df = pd.DataFrame(vectors_test.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f9c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9371f9be",
   "metadata": {},
   "source": [
    "## 学習=>予測=>精度を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_pred_accuracy(clf, train_x, train_y, test_x, test_y):\n",
    "  # 学習\n",
    "  clf.fit(train_x, train_y)\n",
    "  # 予測\n",
    "  predict_y = clf.predict(test_x)\n",
    "  # 正解率\n",
    "  print('True:', np.count_nonzero(predict_y == test_y))\n",
    "  print('False:', np.count_nonzero(predict_y != test_y))\n",
    "  print('正解率;', np.count_nonzero(predict_y == test_y) / len(test_y))\n",
    "  # 様々な精度結果\n",
    "  print(classification_report(test_y, predict_y))\n",
    "  # 正解と予測データ\n",
    "  print('正解：', test_y)\n",
    "  print('予測：', predict_y)\n",
    "  # 予測データのヒストグラム\n",
    "  plt.hist(predict_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71490e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 各値をarray化\n",
    "train_x = np.array(train_text_df)\n",
    "train_y = np.array(train_df.target)\n",
    "test_x = np.array(test_text_df)\n",
    "test_y = np.array(test_df.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03330d0",
   "metadata": {},
   "source": [
    "## 正解データのヒストグラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee73bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6910f3",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb088790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "learn_pred_accuracy(clf, train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d920f",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a51d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=144)\n",
    "learn_pred_accuracy(clf, train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb3db5",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0435431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier(random_state=144, objective='binary:logistic')\n",
    "learn_pred_accuracy(clf, train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681718b1",
   "metadata": {},
   "source": [
    "## ロジスティック回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef6d203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=144, max_iter=144000)\n",
    "learn_pred_accuracy(clf, train_x, train_y, test_x, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
