{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "livedoor_news_multi_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mrmVSI2aqFV"
      },
      "source": [
        "## データ準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef0wdwDUada8"
      },
      "source": [
        "import urllib.request\n",
        "import sys\n",
        "\n",
        "#ダウンロード\n",
        "url = \"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\"\n",
        "urllib.request.urlretrieve(url,\"ldcc-20140209.tar.gz\")\n",
        "# 解凍\n",
        "!tar xvf ldcc-20140209.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdbfo7APbm-e"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "import linecache\n",
        "\n",
        "# カテゴリを配列で取得\n",
        "categories = [name for name in os.listdir('text') if os.path.isdir(f'text/{name}')]\n",
        "print('カテゴリ：', categories)\n",
        "\n",
        "title_list = []\n",
        "body_list = []\n",
        "cat_list = []\n",
        "for category in categories:\n",
        "  path = f'text/{category}/*.txt'\n",
        "  files = glob(path)\n",
        "  for text_name in files:\n",
        "    # 各テキストの3行目をリストに追加（タイトル）\n",
        "    title_list.append(linecache.getline(text_name, 3))\n",
        "    # 各テキストの4〜10行目をリストに追加（本文）\n",
        "    body = ''\n",
        "    for body_line in range(4, 10):\n",
        "      body += linecache.getline(text_name, body_line)\n",
        "    body_list.append(body)\n",
        "    # 各テキストのカテゴリーをリストに追加\n",
        "    cat_list.append(category)\n",
        "\n",
        "cols = {'TITLE': title_list, 'BODY': body_list, 'CATEGORY': cat_list}\n",
        "df = pd.DataFrame(cols)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g77dtYFmh76c"
      },
      "source": [
        "## データを眺める"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWuV1O3Xh9qU"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCA9sq9liAxv"
      },
      "source": [
        "df['CATEGORY'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEwY1lnx0L-l"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "df['CATEGORY'].value_counts().plot(kind='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBpoTi9w1iPW"
      },
      "source": [
        "df['CATEGORY'].value_counts().plot.kde()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSwKcxWWhwlX"
      },
      "source": [
        "## 前処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr2U5mLheygJ"
      },
      "source": [
        "import re\n",
        "\n",
        "def preprocessing(text):\n",
        "  # 全角 => 半角\n",
        "  text = text.translate(str.maketrans({chr(0xFF01 + i): chr(0x21 + i) for i in range(94)}))\n",
        "  # 英語大文字を小文字化\n",
        "  text = text.lower()\n",
        "  # 削除する文字列\n",
        "  remove_list = ['\\n', '　', ' ']\n",
        "  for rem_word in remove_list:\n",
        "    text = text.replace(rem_word, '')\n",
        "  # 数字列を0に置換\n",
        "  text = re.sub('[0-9]+', '0', text)\n",
        "\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljVSvq3ropqp"
      },
      "source": [
        "df['TITLE'] = df['TITLE'].map(preprocessing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohpAh6TUovoK"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wkQaL0GpCLC"
      },
      "source": [
        "## Mecabインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r9wrIl6oxFi"
      },
      "source": [
        "!apt install aptitude\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.7\n",
        "\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n\n",
        "\n",
        "!sed -e \"s!/var/lib/mecab/dic/debian!/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd!g\" /etc/mecabrc &gt; /etc/mecabrc.new\n",
        "!cp /etc/mecabrc /etc/mecabrc.org\n",
        "!cp /etc/mecabrc.new /etc/mecabrc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwALJcKmpq5t"
      },
      "source": [
        "## 分かち書き"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCMDZ8TlpGpj"
      },
      "source": [
        "import MeCab\n",
        "\n",
        "def tokenize(text, target_pos=['名詞', '形容詞', '形容動詞', '動詞', '副詞']):\n",
        "  tokens = []\n",
        "  mecab = MeCab.Tagger ('-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd')\n",
        "  mecab.parse('') #文字列がGCされるのを防ぐ\n",
        "  node = mecab.parseToNode(text)\n",
        "  while node:\n",
        "    #単語を取得\n",
        "    word = node.surface\n",
        "    #品詞を取得\n",
        "    pos = node.feature.split(\",\")[0]\n",
        "    # 名詞の場合のみ抽出\n",
        "    if pos in target_pos:\n",
        "      tokens.append(word)\n",
        "    #次の単語に進める\n",
        "    node = node.next\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su0bhEH8pxgm"
      },
      "source": [
        "df['TITLE_WAKATI'] = df['TITLE'].map(lambda x: ' '.join(tokenize(x)))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HWVBZiVq_x8"
      },
      "source": [
        "## 目的変数をIDに変換"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d96yuslfrFL-"
      },
      "source": [
        "ctgs_dict = {}\n",
        "for idx, ctg in enumerate(categories):\n",
        "  ctgs_dict[ctg] = idx\n",
        "\n",
        "df['CATEGORY_CODE'] = df['CATEGORY'].map(ctgs_dict)\n",
        "df[::777]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRrWJo2wqprP"
      },
      "source": [
        "## 単語埋め込みのために、学習・検証・テストに分割"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2keh4_fp51I"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, valid_test = train_test_split(df, test_size=0.2, random_state=144, stratify=df['CATEGORY_CODE'])\n",
        "valid, test = train_test_split(valid_test, test_size=0.5, random_state=144, stratify=valid_test['CATEGORY_CODE'])\n",
        "\n",
        "# 事例数の確認\n",
        "print('===学習データ===')\n",
        "print(train['CATEGORY_CODE'].value_counts())\n",
        "print('===検証データ===')\n",
        "print(valid['CATEGORY_CODE'].value_counts())\n",
        "print('===評価データ===')\n",
        "print(test['CATEGORY_CODE'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2vfKvrHsay7"
      },
      "source": [
        "## Word2Vecによる単語埋め込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-HS0AbqqzVd"
      },
      "source": [
        "wakati = train[['TITLE_WAKATI']]\n",
        "wakati.to_csv('wakati.csv', sep='\\n', header=True, index=False)\n",
        "wakati"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "273QL5Xvsjlv"
      },
      "source": [
        "from gensim.models import word2vec\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "sentence_data = word2vec.LineSentence('wakati.csv')\n",
        "vectorizer = word2vec.Word2Vec(sentence_data,\n",
        "                         sg=1,        # Skip-gram\n",
        "                         size=200,    # 次元数\n",
        "                         min_count=1, # min_count回未満の単語を破棄\n",
        "                         window=3,    # 文脈の最大単語数\n",
        "                         hs=1,        # 階層ソフトマックス(ネガティブサンプリングするなら0)\n",
        "                         negative=5,  # ネガティブサンプリング\n",
        "                         iter=50      # Epoch数\n",
        "                         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFOiPuhFsqB6"
      },
      "source": [
        "vectorizer.most_similar('映画')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKZUd149tA64"
      },
      "source": [
        "## 単語埋め込みから特徴ベクトル作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whMZsjYAs2Wp"
      },
      "source": [
        "import string\n",
        "import torch\n",
        "\n",
        "def transform_w2v(text):\n",
        "  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "  words = text.translate(table).split()  # 記号をスペースに置換後、スペースで分割してリスト化\n",
        "  vec = [vectorizer[word] for word in words if word in vectorizer]  # 1語ずつベクトル化\n",
        "\n",
        "  return torch.tensor(sum(vec) / len(vec))  # 平均ベクトルをTensor型に変換して出力"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st77vENWtIMh"
      },
      "source": [
        "# 特徴ベクトルの作成\n",
        "X_train = torch.stack([transform_w2v(text) for text in train['TITLE_WAKATI']])\n",
        "X_valid = torch.stack([transform_w2v(text) for text in valid['TITLE_WAKATI']])\n",
        "X_test = torch.stack([transform_w2v(text) for text in test['TITLE_WAKATI']])\n",
        "\n",
        "print(X_train.size())\n",
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50osXLQRzOPX"
      },
      "source": [
        "## ラベルベクトルの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xan4JIMYysCv"
      },
      "source": [
        "y_train = torch.tensor(train['CATEGORY_CODE'].values)\n",
        "y_valid = torch.tensor(valid['CATEGORY_CODE'].values)\n",
        "y_test = torch.tensor(test['CATEGORY_CODE'].values)\n",
        "\n",
        "print(y_train.size())\n",
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zV8XQCdz6SP"
      },
      "source": [
        "## 学習 => 予測 => 精度評価"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZutRcfszadV"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "def learn_pred_accuracy(clf, train_x, train_y, valid_x, valid_y, test_x, test_y):\n",
        "  # 学習\n",
        "  clf.fit(train_x, train_y)\n",
        "  # 予測\n",
        "  predict_vy = clf.predict(valid_x)\n",
        "  predict_y = clf.predict(test_x)\n",
        "  # 正解率\n",
        "  print('検証True:', np.count_nonzero(predict_vy == valid_y))\n",
        "  print('検証False:', np.count_nonzero(predict_vy != valid_y))\n",
        "  print('検証正解率:', np.count_nonzero(predict_vy == valid_y) / len(valid_y))\n",
        "  print('テストTrue:', np.count_nonzero(predict_y == test_y))\n",
        "  print('テストFalse:', np.count_nonzero(predict_y != test_y))\n",
        "  print('テスト正解率:', np.count_nonzero(predict_y == test_y) / len(test_y))\n",
        "  # 様々な精度結果\n",
        "  print(classification_report(test_y, predict_y))\n",
        "  # 正解と予測データ\n",
        "  print('検証正解：', valid_y[:20])\n",
        "  print('検証予測：', predict_vy[:20])\n",
        "  print('test正解：', test_y[:20])\n",
        "  print('test予測：', predict_y[:20])\n",
        "  # 予測データのヒストグラム\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "  sns.countplot(x=predict_vy, ax=ax1)\n",
        "  sns.countplot(x=predict_y, ax=ax2)\n",
        "  # 混合行列\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17, 5))\n",
        "  cm = confusion_matrix(valid_y, predict_vy)\n",
        "  sns.heatmap(cm, cmap='jet', annot=True, ax=ax1)\n",
        "  cm = confusion_matrix(test_y, predict_y)\n",
        "  sns.heatmap(cm, cmap='jet', annot=True, ax=ax2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAtTVWyH1zvo"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0Me4O7K0GOd"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import preprocessing\n",
        "\n",
        "clf = MultinomialNB(alpha=.01)\n",
        "mm = preprocessing.MinMaxScaler()\n",
        "X_train_mm = mm.fit_transform(X_train)\n",
        "X_valid_mm = mm.transform(X_valid)\n",
        "X_test_mm = mm.transform(X_test)\n",
        "mm = preprocessing.MinMaxScaler()\n",
        "learn_pred_accuracy(clf, X_train_mm, y_train, X_valid_mm, np.array(y_valid), X_test_mm, np.array(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3MTODtp2UNK"
      },
      "source": [
        "## Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e73zA5oH11pu"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=144)\n",
        "learn_pred_accuracy(clf, X_train, y_train, X_valid, np.array(y_valid), X_test, np.array(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMNjYOTn2cO8"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqKOQSBO2XUj"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "clf = XGBClassifier(random_state=144, objective='binary:logistic')\n",
        "learn_pred_accuracy(clf, X_train, y_train, X_valid, np.array(y_valid), X_test, np.array(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtSzV6i821kB"
      },
      "source": [
        "## ロジスティック回帰"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7a3FSez2dm8"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(random_state=144, max_iter=144000)\n",
        "learn_pred_accuracy(clf, X_train, y_train, X_valid, np.array(y_valid), X_test, np.array(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf9RQtdZ3Kv7"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGEDRKwf23OE"
      },
      "source": [
        "# バッチサイズを1とする\n",
        "\"\"\"\n",
        "X_train = X_train.view(len(X_train), 1, -1)\n",
        "X_valid = X_valid.view(len(X_valid), 1, -1)\n",
        "X_test = X_test.view(len(X_test), 1, -1)\n",
        "\n",
        "print('X_train:', X_train.size())\n",
        "print('X_valid:', X_valid.size())\n",
        "print('X_test:', X_test.size())\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q0IM3hh4Aq2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# nn.Moduleを継承して新しいクラスを作る。決まり文句\n",
        "class LSTMClassifier(nn.Module):\n",
        "  # モデルで使う各ネットワークをコンストラクタで定義\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "    # 親クラスのコンストラクタ。決まり文句\n",
        "    super(LSTMClassifier, self).__init__()\n",
        "    # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。\n",
        "    self.hidden_dim = hidden_dim\n",
        "    # インプットの単語をベクトル化するために使う\n",
        "    self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "    # LSTMの隠れ層。これ１つでOK。超便利。\n",
        "    self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "    # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
        "    self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "    # softmaxのLog版。dim=0で列、dim=1で行方向を確率変換。\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  # 順伝播処理はforward関数に記載\n",
        "  def forward(self, sentence):\n",
        "    # 文章内の各単語をベクトル化して出力。2次元のテンソル\n",
        "    embeds = self.word_embeddings(sentence)\n",
        "    # 2次元テンソルをLSTMに食わせられる様にviewで３次元テンソルにした上でLSTMへ流す。\n",
        "    # 上記で説明した様にmany to oneのタスクを解きたいので、第二戻り値だけ使う。\n",
        "    _, lstm_out = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "    # lstm_out[0]は３次元テンソルになってしまっているので2次元に調整して全結合。\n",
        "    tag_space = self.hidden2tag(lstm_out[0].view(-1, self.hidden_dim))\n",
        "    # softmaxに食わせて、確率として表現\n",
        "    tag_scores = self.softmax(tag_space)\n",
        "    return tag_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1r73AfG5ySu"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# trainとtestに分割\n",
        "traindata, testdata = train_test_split(df, train_size=0.8)\n",
        "# 単語のベクトル次元数\n",
        "EMBEDDING_DIM = 10\n",
        "# 隠れ層の次元数\n",
        "HIDDEN_DIM = 128\n",
        "# データ全体の単語数\n",
        "VOCAB_SIZE = len(word2index)\n",
        "# 分類先のカテゴリの数\n",
        "TAG_SIZE = len(categories)\n",
        "# モデル宣言\n",
        "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE, TAG_SIZE)\n",
        "# 損失関数はNLLLoss()を使う。LogSoftmaxを使う時はこれを使うらしい。\n",
        "loss_function = nn.NLLLoss()\n",
        "# 最適化の手法はSGDで。lossの減りに時間かかるけど、一旦はこれを使う。\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "def make_wakati(sentence):\n",
        "  # MeCabで分かち書き\n",
        "  mecab = MeCab.Tagger ('-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd')\n",
        "  mecab.parse('') #文字列がGCされるのを防ぐ\n",
        "  sentence = mecab.parse(sentence)\n",
        "  # 半角全角英数字除去\n",
        "  sentence = re.sub(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+', \" \", sentence)\n",
        "  # 記号もろもろ除去\n",
        "  sentence = re.sub(r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+', \"\", sentence)\n",
        "  # スペースで区切って形態素の配列へ\n",
        "  wakati = sentence.split(\" \")\n",
        "  # 空の要素は削除\n",
        "  wakati = list(filter((\"\").__ne__, wakati))\n",
        "  return wakati\n",
        "\n",
        "# 単語ID辞書を作成する\n",
        "word2index = {}\n",
        "for title in df['TITLE']:\n",
        "  wakati = make_wakati(title)\n",
        "  for word in wakati:\n",
        "    if word in word2index: continue\n",
        "    word2index[word] = len(word2index)\n",
        "\n",
        "# 文章を単語IDの系列データに変換\n",
        "# PyTorchのLSTMのインプットになるデータなので、もちろんtensor型で\n",
        "def sentence2index(sentence):\n",
        "  wakati = make_wakati(sentence)\n",
        "  return torch.tensor([word2index[w] for w in wakati], dtype=torch.long)\n",
        "\n",
        "category2index = {}\n",
        "for cat in categories:\n",
        "  if cat in category2index: continue\n",
        "  category2index[cat] = len(category2index)\n",
        "\n",
        "def category2tensor(cat):\n",
        "  return torch.tensor([category2index[cat]], dtype=torch.long)\n",
        "\n",
        "# 各エポックの合計loss値を格納する\n",
        "losses = []\n",
        "# 10ループ回してみる。（バッチ化とかGPU使ってないので結構時間かかる...）\n",
        "for epoch in range(10):\n",
        "    all_loss = 0\n",
        "    for title, cat in zip(traindata['TITLE'], traindata['CATEGORY']):\n",
        "        # モデルが持ってる勾配の情報をリセット\n",
        "        model.zero_grad()\n",
        "        # 文章を単語IDの系列に変換（modelに食わせられる形に変換）\n",
        "        inputs = sentence2index(title)\n",
        "        # 順伝播の結果を受け取る\n",
        "        out = model(inputs)\n",
        "        # 正解カテゴリをテンソル化\n",
        "        answer = category2tensor(cat)\n",
        "        # 正解とのlossを計算\n",
        "        loss = loss_function(out, answer)\n",
        "        # 勾配をセット\n",
        "        loss.backward()\n",
        "        # 逆伝播でパラメータ更新\n",
        "        optimizer.step()\n",
        "        # lossを集計\n",
        "        all_loss += loss.item()\n",
        "    losses.append(all_loss)\n",
        "    print('epoch', epoch, '\\t' , 'loss', all_loss)\n",
        "print('done.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYtH134x6dZ7"
      },
      "source": [
        "plt.plot(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MVQ8q5F9CWy"
      },
      "source": [
        "# テストデータの母数計算\n",
        "test_num = len(testdata)\n",
        "# 正解の件数\n",
        "a = 0\n",
        "# 勾配自動計算OFF\n",
        "with torch.no_grad():\n",
        "    for title, category in zip(testdata['TITLE'], testdata['CATEGORY']):\n",
        "        # テストデータの予測\n",
        "        inputs = sentence2index(title)\n",
        "        out = model(inputs)\n",
        "\n",
        "        # outの一番大きい要素を予測結果をする\n",
        "        _, predict = torch.max(out, 1)\n",
        "\n",
        "        answer = category2tensor(category)\n",
        "        if predict == answer:\n",
        "            a += 1\n",
        "print(\"predict : \", a / test_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Cdw2zph-5qI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}